{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9996978,"sourceType":"datasetVersion","datasetId":6152943},{"sourceId":11731671,"sourceType":"datasetVersion","datasetId":7364596}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import wandb\nwandb.login(key=\"5fb34431b405eb21dc0f263e5b3cf2c15fdc7471\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T11:22:00.532411Z","iopub.execute_input":"2025-05-20T11:22:00.532636Z","iopub.status.idle":"2025-05-20T11:22:09.684815Z","shell.execute_reply.started":"2025-05-20T11:22:00.532612Z","shell.execute_reply":"2025-05-20T11:22:09.684242Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mma24m004\u001b[0m (\u001b[33mma24m004-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"# !pip install yattag\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T04:36:22.649845Z","iopub.execute_input":"2025-05-20T04:36:22.650689Z","iopub.status.idle":"2025-05-20T04:36:25.799326Z","shell.execute_reply.started":"2025-05-20T04:36:22.650653Z","shell.execute_reply":"2025-05-20T04:36:25.798533Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: yattag in /usr/local/lib/python3.11/dist-packages (1.16.1)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nfrom torch.nn.utils.rnn import pad_sequence  # <-- ADD THIS LINE\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T11:22:26.834024Z","iopub.execute_input":"2025-05-20T11:22:26.834755Z","iopub.status.idle":"2025-05-20T11:22:26.838392Z","shell.execute_reply.started":"2025-05-20T11:22:26.834729Z","shell.execute_reply":"2025-05-20T11:22:26.837645Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class Attn_Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.rnn = getattr(nn, cell_type)(emb_dim, hidden_dim, \n                                        n_layers, dropout=dropout, \n                                        batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))\n        outputs, hidden = self.rnn(embedded)\n        if isinstance(hidden, tuple):  # LSTM\n            return outputs, hidden[0].contiguous(), hidden[1].contiguous()\n        else:  # GRU/RNN\n            return outputs, hidden.contiguous(), None\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T11:22:30.340705Z","iopub.execute_input":"2025-05-20T11:22:30.341405Z","iopub.status.idle":"2025-05-20T11:22:30.349394Z","shell.execute_reply.started":"2025-05-20T11:22:30.341370Z","shell.execute_reply":"2025-05-20T11:22:30.348624Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.W = nn.Linear(hidden_dim, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n\n    def forward(self, decoder_hidden, encoder_outputs):\n        # decoder_hidden: (batch_size, hidden_dim)\n        # encoder_outputs: (batch_size, seq_len, hidden_dim)\n        \n        # Repeat decoder hidden for each encoder time step\n        decoder_hidden = decoder_hidden.unsqueeze(1).expand_as(encoder_outputs)\n        \n        # Calculate attention energies\n        energy = torch.tanh(self.W(decoder_hidden + encoder_outputs))\n        attention = self.V(energy).squeeze(2)\n        \n        return torch.softmax(attention, dim=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T11:22:33.017519Z","iopub.execute_input":"2025-05-20T11:22:33.018260Z","iopub.status.idle":"2025-05-20T11:22:33.022770Z","shell.execute_reply.started":"2025-05-20T11:22:33.018237Z","shell.execute_reply":"2025-05-20T11:22:33.022106Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class Attn_Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, cell_type, dropout):\n        super().__init__()\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n        self.attention = Attention(hidden_dim)\n        \n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.rnn = getattr(nn, cell_type)(emb_dim + hidden_dim, hidden_dim, \n                                        n_layers, dropout=dropout, batch_first=True)\n        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input, hidden, cell, encoder_outputs):\n        input = input.unsqueeze(1)  # (batch_size, 1)\n        embedded = self.dropout(self.embedding(input))  # (batch_size, 1, emb_dim)\n    \n        # Calculate attention weights: (batch_size, src_len)\n        attn_weights = self.attention(hidden[-1], encoder_outputs)\n    \n        # Create context vector: (batch_size, 1, hidden_dim)\n        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n    \n        # Concatenate context and embedded: (batch_size, 1, emb_dim + hidden_dim)\n        rnn_input = torch.cat([embedded, context], dim=2)\n    \n        # Pass through RNN\n        if cell is not None:\n            output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n        else:\n            output, hidden = self.rnn(rnn_input, hidden)\n            cell = None  # To ensure cell is always returned\n    \n        # Generate prediction\n        output = torch.cat([output.squeeze(1), context.squeeze(1)], dim=1)  # (batch_size, hidden_dim * 2)\n        prediction = self.fc_out(output)  # (batch_size, output_dim)\n    \n        return prediction, hidden, cell, attn_weights  # <-- include attn_weights\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T11:22:35.082268Z","iopub.execute_input":"2025-05-20T11:22:35.083042Z","iopub.status.idle":"2025-05-20T11:22:35.089677Z","shell.execute_reply.started":"2025-05-20T11:22:35.083017Z","shell.execute_reply":"2025-05-20T11:22:35.088979Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nimport numpy as np\nimport pandas as pd\n\n\n# Seq2Seq Model with Beam Search\nclass Attn_Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device, sos_idx, eos_idx):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n        self.sos_idx = sos_idx\n        self.eos_idx = eos_idx\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size = src.shape[0]\n        trg_len = trg.shape[1]\n        \n        # ✅ Fix here (only 3 returned)\n        encoder_outputs, hidden, cell = self.encoder(src)\n        \n        inputs = trg[:, 0]\n        outputs = torch.zeros(batch_size, trg_len, self.decoder.output_dim).to(self.device)\n    \n        for t in range(1, trg_len):\n            # Decoder returns 4, we only care about first 3 during training\n            output, hidden, cell, _ = self.decoder(inputs, hidden, cell, encoder_outputs)\n            outputs[:, t] = output\n            teacher_force = np.random.random() < teacher_forcing_ratio\n            inputs = trg[:, t] if teacher_force else output.argmax(1)\n            \n        return outputs\n\n\n    def predict(self, src, max_len=30, beam_size=3):\n        self.eval()\n        batch_size = src.size(0)\n        assert batch_size == 1, \"Beam search only supports batch_size=1\"\n    \n        with torch.no_grad():\n            encoder_outputs, hidden, cell = self.encoder(src)\n    \n            beams = [(torch.tensor([self.sos_idx], device=self.device), 0.0, hidden, cell, [])]\n            completed_sequences = []\n    \n            for _ in range(max_len):\n                new_beams = []\n                for seq, score, h, c, attn_list in beams:\n                    last_token = seq[-1].unsqueeze(0)\n                    if last_token.item() == self.eos_idx:\n                        completed_sequences.append((seq, score, attn_list))  # ✅ 3-tuple\n                        continue\n    \n                    output, h_new, c_new, attn_weights = self.decoder(last_token, h, c, encoder_outputs)\n                    log_probs = torch.log_softmax(output, dim=1).squeeze(0)\n                    topk = torch.topk(log_probs, beam_size)\n    \n                    for idx, log_prob in zip(topk.indices, topk.values):\n                        new_seq = torch.cat([seq, idx.unsqueeze(0)])\n                        new_score = score + log_prob.item()\n                        new_attn_list = attn_list + [attn_weights.cpu()]\n                        new_beams.append((new_seq, new_score, h_new, c_new, new_attn_list))\n    \n                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n    \n                if all(seq[-1].item() == self.eos_idx for seq, _, _, _, _ in beams):\n                    completed_sequences.extend([\n                        (seq, score, attn_list) for seq, score, _, _, attn_list in beams\n                    ])\n                    break\n    \n            # ✅ Fix: Ensure 3-tuple fallback\n            if not completed_sequences:\n                completed_sequences = [\n                    (seq, score, attn_list) for seq, score, _, _, attn_list in beams\n                ]\n    \n            best_seq, _, best_attn_list = max(completed_sequences, key=lambda x: x[1])\n            return best_seq[1:], best_attn_list  # Remove <SOS> token\n    \n\n# Dataset\nclass DakshinaDataset(Dataset):\n    def __init__(self, data, input_vocab, output_vocab):\n        self.data = data\n        self.input_vocab = input_vocab\n        self.output_vocab = output_vocab\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        src = [self.input_vocab.get(c, self.input_vocab['<UNK>']) for c in self.data.iloc[idx, 1]] + [self.input_vocab['<EOS>']]\n        tgt = [self.output_vocab['<SOS>']] + [self.output_vocab.get(c, self.output_vocab['<UNK>']) for c in self.data.iloc[idx, 0]] + [self.output_vocab['<EOS>']]\n        return torch.tensor(src, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)\n\n\n# Vocab Creation\ndef create_vocab(data, column):\n    vocab = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n    for seq in data[column]:\n        if pd.notna(seq):\n            for char in seq:\n                if char not in vocab:\n                    vocab[char] = len(vocab)\n    return vocab\n\n\n# Collate Function\ndef pad_collate(batch):\n    src_batch, tgt_batch = zip(*batch)\n    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=0)\n    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n    return src_padded, tgt_padded\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T11:22:37.503226Z","iopub.execute_input":"2025-05-20T11:22:37.503497Z","iopub.status.idle":"2025-05-20T11:22:37.518827Z","shell.execute_reply.started":"2025-05-20T11:22:37.503477Z","shell.execute_reply":"2025-05-20T11:22:37.518127Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def Attn_train_model(config=None):\n    with wandb.init(config=config):\n        config = wandb.config\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n        # Load data\n        train_data = pd.read_csv('/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv', sep='\\t', header=None, dtype=str).dropna()\n        dev_data = pd.read_csv('/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv', sep='\\t', header=None, dtype=str).dropna()\n\n        # Create vocabularies\n        input_vocab = create_vocab(train_data, 1)  # English (source)\n        output_vocab = create_vocab(train_data, 0)  # Hindi (target)\n\n        # Prepare datasets and loaders\n        train_dataset = DakshinaDataset(train_data, input_vocab, output_vocab)\n        dev_dataset = DakshinaDataset(dev_data, input_vocab, output_vocab)\n        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=pad_collate)\n        dev_loader = DataLoader(dev_dataset, batch_size=32, shuffle=False, collate_fn=pad_collate)\n\n        # Initialize model\n        encoder = Attn_Encoder(len(input_vocab), config.emb_dim, config.hidden_dim, config.num_layers, config.cell_type, config.dropout)\n        decoder = Attn_Decoder(len(output_vocab), config.emb_dim, config.hidden_dim, config.num_layers, config.cell_type, config.dropout)\n        model = Attn_Seq2Seq(\n            encoder, decoder, device,\n            sos_idx=output_vocab['<SOS>'],\n            eos_idx=output_vocab['<EOS>']\n        ).to(device)\n\n        # Loss and optimizer\n        optimizer = optim.Adam(model.parameters(), lr=0.001)\n        criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n        best_val_accuracy = 0\n        patience_counter = 0\n\n        # Training loop\n        for epoch in range(config.epochs):\n            model.train()\n            train_loss, train_correct, train_total = 0, 0, 0\n            for src, tgt in train_loader:\n                src, tgt = src.to(device), tgt.to(device)\n                optimizer.zero_grad()\n                output = model(src, tgt)\n                output = output[:, 1:].reshape(-1, output.shape[-1])\n                tgt = tgt[:, 1:].reshape(-1)\n                loss = criterion(output, tgt)\n                loss.backward()\n                optimizer.step()\n\n                train_loss += loss.item()\n                preds = output.argmax(dim=1)\n                non_pad_mask = tgt != 0\n                train_correct += (preds == tgt)[non_pad_mask].sum().item()\n                train_total += non_pad_mask.sum().item()\n\n            train_accuracy = train_correct / train_total\n\n            # Validation\n            model.eval()\n            val_loss, val_correct, val_total = 0, 0, 0\n            with torch.no_grad():\n                for src, tgt in dev_loader:\n                    src, tgt = src.to(device), tgt.to(device)\n                    output = model(src, tgt, teacher_forcing_ratio=0)\n                    output = output[:, 1:].reshape(-1, output.shape[-1])\n                    tgt = tgt[:, 1:].reshape(-1)\n                    loss = criterion(output, tgt)\n                    val_loss += loss.item()\n                    preds = output.argmax(dim=1)\n                    non_pad_mask = tgt != 0\n                    val_correct += (preds == tgt)[non_pad_mask].sum().item()\n                    val_total += non_pad_mask.sum().item()\n\n            val_accuracy = val_correct / val_total\n\n            # Save best model\n            if val_accuracy > best_val_accuracy:\n                best_val_accuracy = val_accuracy\n                torch.save(model.state_dict(), '/kaggle/working/Attn_best_model.pt')\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                if patience_counter >= 4:\n                    print(\"Early stopping triggered.\")\n                    break\n\n            # Log metrics to wandb\n            wandb.log({\n                'epoch': epoch,\n                'train_loss': train_loss / len(train_loader),\n                'val_loss': val_loss / len(dev_loader),\n                'train_accuracy': train_accuracy,\n                'val_accuracy': val_accuracy\n            })\n\n            # Log sample predictions\n            src_sample, tgt_sample = next(iter(dev_loader))\n            src_sample, tgt_sample = src_sample.to(device), tgt_sample.to(device)\n            preds = []\n            for i in range(5):\n                single_input = src_sample[i].unsqueeze(0)\n                pred_seq, _ = model.predict(single_input, beam_size=config.beam_size)  # <-- FIX: unpack properly\n                preds.append(pred_seq)\n\n            inv_input_vocab = {v: k for k, v in input_vocab.items()}\n            inv_output_vocab = {v: k for k, v in output_vocab.items()}\n            table = wandb.Table(columns=[\"Input\", \"Target\", \"Prediction\"])\n            for i in range(len(preds)):\n                input_str = ''.join([inv_input_vocab.get(id.item(), '?') for id in src_sample[i] if id.item() not in [0, input_vocab['<EOS>']]])\n                target_str = ''.join([inv_output_vocab.get(id.item(), '?') for id in tgt_sample[i] if id.item() not in [0, output_vocab['<EOS>']]])\n                \n                pred_tokens = []\n                for idx in preds[i]:\n                    idx_val = idx.item() if isinstance(idx, torch.Tensor) else idx\n                    if idx_val == output_vocab['<EOS>']:\n                        break\n                    pred_tokens.append(inv_output_vocab.get(idx_val, '?'))\n                pred_str = ''.join(pred_tokens)\n\n                table.add_data(input_str, target_str, pred_str)\n\n            wandb.log({\"Predictions\": table})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T11:22:41.613584Z","iopub.execute_input":"2025-05-20T11:22:41.614260Z","iopub.status.idle":"2025-05-20T11:22:41.629019Z","shell.execute_reply.started":"2025-05-20T11:22:41.614238Z","shell.execute_reply":"2025-05-20T11:22:41.628331Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\n# Sweep\nsweep_config = {\n    'method': 'bayes',\n    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n    'parameters': {\n        'emb_dim': {'values': [16, 32, 64, 256]},\n        'hidden_dim': {'values': [16, 32, 64, 256]},\n        'num_layers': {'values': [1, 2, 3]},\n        'cell_type': {'values': ['RNN', 'GRU', 'LSTM']},\n        'dropout': {'values': [0.2, 0.3]},\n        'epochs': {'values': [10, 5]},\n        'beam_size': {'values': [1, 3, 5]}\n    }\n}\n\nsweep_id = wandb.sweep(sweep_config, project='DL_ASSIGNMENT_3_RNN')\nwandb.agent(sweep_id, Attn_train_model, count=10)\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T11:47:41.850804Z","iopub.execute_input":"2025-05-20T11:47:41.851073Z","iopub.status.idle":"2025-05-20T11:47:41.856575Z","shell.execute_reply.started":"2025-05-20T11:47:41.851052Z","shell.execute_reply":"2025-05-20T11:47:41.855628Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"!fc-list | grep Devanagari","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T04:50:18.000441Z","iopub.execute_input":"2025-05-20T04:50:18.001036Z","iopub.status.idle":"2025-05-20T04:50:18.170141Z","shell.execute_reply.started":"2025-05-20T04:50:18.001013Z","shell.execute_reply":"2025-05-20T04:50:18.169326Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"!wget https://noto-website-2.storage.googleapis.com/pkgs/NotoSansDevanagari-hinted.zip\n!unzip NotoSansDevanagari-hinted.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T04:50:19.889063Z","iopub.execute_input":"2025-05-20T04:50:19.889670Z","iopub.status.idle":"2025-05-20T04:50:20.629666Z","shell.execute_reply.started":"2025-05-20T04:50:19.889640Z","shell.execute_reply":"2025-05-20T04:50:20.628914Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"--2025-05-20 04:50:19--  https://noto-website-2.storage.googleapis.com/pkgs/NotoSansDevanagari-hinted.zip\nResolving noto-website-2.storage.googleapis.com (noto-website-2.storage.googleapis.com)... 173.194.210.207, 108.177.12.207, 142.251.107.207, ...\nConnecting to noto-website-2.storage.googleapis.com (noto-website-2.storage.googleapis.com)|173.194.210.207|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 6201435 (5.9M) [application/zip]\nSaving to: ‘NotoSansDevanagari-hinted.zip’\n\nNotoSansDevanagari- 100%[===================>]   5.91M  --.-KB/s    in 0.08s   \n\n2025-05-20 04:50:20 (75.2 MB/s) - ‘NotoSansDevanagari-hinted.zip’ saved [6201435/6201435]\n\nArchive:  NotoSansDevanagari-hinted.zip\n  inflating: LICENSE_OFL.txt         \n  inflating: NotoSansDevanagari-Black.ttf  \n  inflating: NotoSansDevanagari-Bold.ttf  \n  inflating: NotoSansDevanagari-Condensed.ttf  \n  inflating: NotoSansDevanagari-CondensedBlack.ttf  \n  inflating: NotoSansDevanagari-CondensedBold.ttf  \n  inflating: NotoSansDevanagari-CondensedExtraBold.ttf  \n  inflating: NotoSansDevanagari-CondensedExtraLight.ttf  \n  inflating: NotoSansDevanagari-CondensedLight.ttf  \n  inflating: NotoSansDevanagari-CondensedMedium.ttf  \n  inflating: NotoSansDevanagari-CondensedSemiBold.ttf  \n  inflating: NotoSansDevanagari-CondensedThin.ttf  \n  inflating: NotoSansDevanagari-ExtraBold.ttf  \n  inflating: NotoSansDevanagari-ExtraCondensed.ttf  \n  inflating: NotoSansDevanagari-ExtraCondensedBlack.ttf  \n  inflating: NotoSansDevanagari-ExtraCondensedBold.ttf  \n  inflating: NotoSansDevanagari-ExtraCondensedExtraBold.ttf  \n  inflating: NotoSansDevanagari-ExtraCondensedExtraLight.ttf  \n  inflating: NotoSansDevanagari-ExtraCondensedLight.ttf  \n  inflating: NotoSansDevanagari-ExtraCondensedMedium.ttf  \n  inflating: NotoSansDevanagari-ExtraCondensedSemiBold.ttf  \n  inflating: NotoSansDevanagari-ExtraCondensedThin.ttf  \n  inflating: NotoSansDevanagari-ExtraLight.ttf  \n  inflating: NotoSansDevanagari-Light.ttf  \n  inflating: NotoSansDevanagari-Medium.ttf  \n  inflating: NotoSansDevanagari-Regular.ttf  \n  inflating: NotoSansDevanagari-SemiBold.ttf  \n  inflating: NotoSansDevanagari-SemiCondensed.ttf  \n  inflating: NotoSansDevanagari-SemiCondensedBlack.ttf  \n  inflating: NotoSansDevanagari-SemiCondensedBold.ttf  \n  inflating: NotoSansDevanagari-SemiCondensedExtraBold.ttf  \n  inflating: NotoSansDevanagari-SemiCondensedExtraLight.ttf  \n  inflating: NotoSansDevanagari-SemiCondensedLight.ttf  \n  inflating: NotoSansDevanagari-SemiCondensedMedium.ttf  \n  inflating: NotoSansDevanagari-SemiCondensedSemiBold.ttf  \n  inflating: NotoSansDevanagari-SemiCondensedThin.ttf  \n  inflating: NotoSansDevanagari-Thin.ttf  \n  inflating: NotoSansDevanagariUI-Black.ttf  \n  inflating: NotoSansDevanagariUI-Bold.ttf  \n  inflating: NotoSansDevanagariUI-Condensed.ttf  \n  inflating: NotoSansDevanagariUI-CondensedBlack.ttf  \n  inflating: NotoSansDevanagariUI-CondensedBold.ttf  \n  inflating: NotoSansDevanagariUI-CondensedExtraBold.ttf  \n  inflating: NotoSansDevanagariUI-CondensedExtraLight.ttf  \n  inflating: NotoSansDevanagariUI-CondensedLight.ttf  \n  inflating: NotoSansDevanagariUI-CondensedMedium.ttf  \n  inflating: NotoSansDevanagariUI-CondensedSemiBold.ttf  \n  inflating: NotoSansDevanagariUI-CondensedThin.ttf  \n  inflating: NotoSansDevanagariUI-ExtraBold.ttf  \n  inflating: NotoSansDevanagariUI-ExtraCondensed.ttf  \n  inflating: NotoSansDevanagariUI-ExtraCondensedBlack.ttf  \n  inflating: NotoSansDevanagariUI-ExtraCondensedBold.ttf  \n  inflating: NotoSansDevanagariUI-ExtraCondensedExtraBold.ttf  \n  inflating: NotoSansDevanagariUI-ExtraCondensedExtraLight.ttf  \n  inflating: NotoSansDevanagariUI-ExtraCondensedLight.ttf  \n  inflating: NotoSansDevanagariUI-ExtraCondensedMedium.ttf  \n  inflating: NotoSansDevanagariUI-ExtraCondensedSemiBold.ttf  \n  inflating: NotoSansDevanagariUI-ExtraCondensedThin.ttf  \n  inflating: NotoSansDevanagariUI-ExtraLight.ttf  \n  inflating: NotoSansDevanagariUI-Light.ttf  \n  inflating: NotoSansDevanagariUI-Medium.ttf  \n  inflating: NotoSansDevanagariUI-Regular.ttf  \n  inflating: NotoSansDevanagariUI-SemiBold.ttf  \n  inflating: NotoSansDevanagariUI-SemiCondensed.ttf  \n  inflating: NotoSansDevanagariUI-SemiCondensedBlack.ttf  \n  inflating: NotoSansDevanagariUI-SemiCondensedBold.ttf  \n  inflating: NotoSansDevanagariUI-SemiCondensedExtraBold.ttf  \n  inflating: NotoSansDevanagariUI-SemiCondensedExtraLight.ttf  \n  inflating: NotoSansDevanagariUI-SemiCondensedLight.ttf  \n  inflating: NotoSansDevanagariUI-SemiCondensedMedium.ttf  \n  inflating: NotoSansDevanagariUI-SemiCondensedSemiBold.ttf  \n  inflating: NotoSansDevanagariUI-SemiCondensedThin.ttf  \n  inflating: NotoSansDevanagariUI-Thin.ttf  \n  inflating: README                  \n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nfrom matplotlib import font_manager, rcParams\n\n# Use 'Agg' backend for non-GUI environments (optional but safe)\nimport matplotlib\nmatplotlib.use(\"Agg\")\n\n# Path to the Noto Sans Devanagari font (adjust if needed)\nfont_path = \"/kaggle/input/notosans-devanagiri/static/NotoSansDevanagari-Regular.ttf\"\n\nif os.path.exists(font_path):\n    font_manager.fontManager.addfont(font_path)\n    dev_font = font_manager.FontProperties(fname=font_path)\n    rcParams['font.family'] = dev_font.get_name()\n    print(f\"✅ Loaded font: {dev_font.get_name()}\")\nelse:\n    print(\"❌ Devanagari font not found. Falling back to default.\")\n    dev_font = None\n\n# Test plot to render \"भारत\"\nplt.figure(figsize=(6, 2))\nplt.text(0.5, 0.5, \"भारत\", fontsize=30, ha='center', fontproperties=dev_font)\nplt.title(\"Test: Devanagari Font Rendering\", fontproperties=dev_font)\nplt.axis('off')\nplt.tight_layout()\nplt.savefig(\"/kaggle/working/devanagari_test.png\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T04:50:26.528962Z","iopub.execute_input":"2025-05-20T04:50:26.529680Z","iopub.status.idle":"2025-05-20T04:50:26.706922Z","shell.execute_reply.started":"2025-05-20T04:50:26.529649Z","shell.execute_reply":"2025-05-20T04:50:26.706033Z"}},"outputs":[{"name":"stdout","text":"✅ Loaded font: Noto Sans Devanagari\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"plt.figure(figsize=(6, 2))\nplt.text(0.5, 0.5, \"भारत\", fontsize=30, fontproperties=dev_font, ha='center')\nplt.axis('off')\nplt.title(\"Test: Devanagari Font Rendering\", fontproperties=dev_font)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T04:50:30.934958Z","iopub.execute_input":"2025-05-20T04:50:30.935302Z","iopub.status.idle":"2025-05-20T04:50:30.975969Z","shell.execute_reply.started":"2025-05-20T04:50:30.935279Z","shell.execute_reply":"2025-05-20T04:50:30.975185Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## **Implementation On Test Data**","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.image as mpimg\nfrom torch.utils.data import DataLoader\nfrom IPython.display import display, HTML\nfrom matplotlib import font_manager, rcParams\nimport wandb\n\n# ============================ #\n#   1. Font Setup for Hindi   #\n# ============================ #\nfont_path = \"/kaggle/input/notosans-devanagiri/static/NotoSansDevanagari-Regular.ttf\"\nassert os.path.exists(font_path), \"Devanagari font not found.\"\ndevanagari_font = font_manager.FontProperties(fname=font_path)\nfont_manager.fontManager.addfont(font_path)\nrcParams['font.family'] = devanagari_font.get_name()\nprint(f\"Using Devanagari font: {devanagari_font.get_name()}\")\n\n# ============================ #\n#   2. Device & W&B Init      #\n# ============================ #\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nwandb.init(project=\"DL_ASSIGNMENT_3_RNN\", name=\"test-inference-attention\")\n\n# ============================ #\n#   3. Load Data & Vocab      #\n# ============================ #\ntrain_path = '/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv'\ntest_path = '/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv'\ntrain_data = pd.read_csv(train_path, sep='\\t', header=None, dtype=str).dropna()\ntest_data = pd.read_csv(test_path, sep='\\t', header=None, dtype=str).dropna()\n\ninput_vocab = create_vocab(train_data, 1)\noutput_vocab = create_vocab(train_data, 0)\ninv_input_vocab = {v: k for k, v in input_vocab.items()}\ninv_output_vocab = {v: k for k, v in output_vocab.items()}\n\ntest_dataset = DakshinaDataset(test_data, input_vocab, output_vocab)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=pad_collate)\n\n# ============================ #\n#   4. Load Trained Model     #\n# ============================ #\nemb_dim = 32\nhidden_dim = 256\nnum_layers = 3\ndropout = 0.3\ncell_type = 'LSTM'\nbeam_size = 3\nsos_idx = output_vocab['<SOS>']\neos_idx = output_vocab['<EOS>']\n\nattention = Attention(hidden_dim)\nencoder = Attn_Encoder(len(input_vocab), emb_dim, hidden_dim, num_layers, cell_type, dropout)\ndecoder = Attn_Decoder(len(output_vocab), emb_dim, hidden_dim, num_layers, cell_type, dropout)\nmodel = Attn_Seq2Seq(encoder, decoder, device, sos_idx, eos_idx).to(device)\n\nmodel_path = '/kaggle/working/Attn_best_model.pt'\nmodel.load_state_dict(torch.load(model_path, map_location=device))\nmodel.eval()\n\n# ============================ #\n#   5. Inference & Viz        #\n# ============================ #\ncorrect_words, total_words = 0, 0\ntotal_correct_chars, total_chars = 0, 0\npredictions = []\nviz_count, viz_samples = 0, 10\n\nwith torch.no_grad():\n    for batch_idx, (src, tgt) in enumerate(test_loader):\n        src, tgt = src.to(device), tgt.to(device)\n\n        for i in range(src.size(0)):\n            src_single = src[i].unsqueeze(0)\n            tgt_single = tgt[i].unsqueeze(0)\n\n            pred_seq, best_attn_list = model.predict(src_single, max_len=30, beam_size=beam_size)\n\n            pred_indices = pred_seq.tolist()\n            if eos_idx in pred_indices:\n                pred_indices = pred_indices[:pred_indices.index(eos_idx)]\n            pred_str = ''.join([inv_output_vocab.get(idx, '?') for idx in pred_indices])\n\n            tgt_indices = tgt_single[0, 1:].tolist()\n            if eos_idx in tgt_indices:\n                tgt_indices = tgt_indices[:tgt_indices.index(eos_idx)]\n            tgt_str = ''.join([inv_output_vocab.get(idx, '?') for idx in tgt_indices])\n\n            input_indices = src_single[0].tolist()\n            input_str = ''.join([inv_input_vocab.get(idx, '?') for idx in input_indices if idx not in [0, input_vocab['<EOS>']]])\n\n            correct_word = pred_str == tgt_str\n            correct_chars = sum(1 for p, t in zip(pred_str, tgt_str) if p == t)\n\n            correct_words += correct_word\n            total_words += 1\n            total_correct_chars += correct_chars\n            total_chars += len(tgt_str)\n\n            predictions.append({\n                'input': input_str,\n                'target': tgt_str,\n                'prediction': pred_str,\n                'correct_word': correct_word,\n                'correct_chars': correct_chars,\n                'total_chars': len(tgt_str)\n            })\n\n            if viz_count < viz_samples:\n                input_chars = [inv_input_vocab.get(idx, '?') for idx in input_indices if idx not in [0, input_vocab['<EOS>']]]\n                pred_chars = [inv_output_vocab.get(idx, '?') for idx in pred_indices]\n                attn_weights = [attn.cpu().numpy()[0] for attn in best_attn_list]\n\n                fig, axes = plt.subplots(len(pred_chars), 1, figsize=(max(6, 0.7 * len(input_chars)), 1.5 * len(pred_chars) + 1))\n                if len(pred_chars) == 1:\n                    axes = [axes]\n\n                fig.suptitle(f\"Input: {input_str} -> Predicted: {pred_str}\", fontsize=14, fontproperties=devanagari_font, y=1.05)\n\n                for j, ax in enumerate(axes):\n                    ax.set_title(f\"Output char: {pred_chars[j]}\", fontproperties=devanagari_font, fontsize=10)\n                    for k, char in enumerate(input_chars):\n                        weight = attn_weights[j][k]\n                        ax.text(k * 0.7, 0, char, fontsize=14, ha='center', va='center',\n                                bbox=dict(facecolor=plt.cm.Greens(weight), alpha=1, edgecolor='none', boxstyle='round,pad=0.2'),\n                                fontproperties=devanagari_font)\n                    ax.set_xlim(-0.5, len(input_chars) * 0.7)\n                    ax.set_ylim(-0.5, 0.5)\n                    ax.axis('off')\n\n                plt.tight_layout(rect=[0, 0, 1, 0.95])\n                viz_path = f'/kaggle/working/attent_@_sample_{viz_count}.png'\n                plt.savefig(viz_path, dpi=150, bbox_inches='tight', pad_inches=0.1)\n                plt.close(fig)\n                wandb.log({f\"attent_@_sample_{viz_count}\": wandb.Image(viz_path)})\n                viz_count += 1\n\n# ============================ #\n#   6. Combine All Images     #\n# ============================ #\nif viz_count > 0:\n    grid_cols = 2\n    grid_rows = (viz_count + grid_cols - 1) // grid_cols\n    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(12, 1.2 * grid_rows * 5))\n\n    for i in range(grid_rows * grid_cols):\n        ax = axes.flat[i]\n        if i < viz_count:\n            img = mpimg.imread(f'/kaggle/working/attent_@_sample_{i}.png')\n            ax.imshow(img)\n            ax.set_title(f\"Sample {i}\", fontsize=12)\n        ax.axis('off')\n\n    plt.tight_layout()\n    combined_path = '/kaggle/working/attent_@_samples_combined.png'\n    plt.savefig(combined_path, dpi=150)\n    plt.close(fig)\n    wandb.log({\"attent_@_samples_combined\": wandb.Image(combined_path)})\n    print(f\"Saved combined attention image: {combined_path}\")\n\n# ============================ #\n#   7. Log Accuracy + Table   #\n# ============================ #\nword_accuracy = 100 * correct_words / total_words\nchar_accuracy = 100 * total_correct_chars / total_chars\nwandb.log({\"test_word_accuracy\": word_accuracy, \"test_char_accuracy\": char_accuracy})\nprint(f\"\\nWord Accuracy: {word_accuracy:.2f}%\")\nprint(f\"Char Accuracy: {char_accuracy:.2f}%\")\n\n# ============================ #\n#   8. Save Predictions       #\n# ============================ #\ndf_predictions = pd.DataFrame(predictions)\ncsv_path = '/kaggle/working/pred_attention.csv'\nhtml_plain_path = '/kaggle/working/predictions_all_plain_attention.html'\nhtml_colored_path = '/kaggle/working/predictions_sample_colored_attention.html'\n\ndf_predictions.to_csv(csv_path, index=False)\ndf_predictions.to_html(html_plain_path, index=False)\n\ndef highlight_row(row):\n    return ['background-color: #d4edda;' if row['correct_word'] else 'background-color: #f8d7da;'] * len(row)\n\nsample_df = pd.DataFrame(predictions[:viz_count])\n\nstyled_sample = sample_df.style.apply(highlight_row, axis=1)\\\n    .set_table_styles([{'selector': 'th', 'props': [('text-align', 'center')]}])\\\n    .set_properties(**{'text-align': 'left', 'padding': '8px', 'font-size': '14px', 'border': '1px solid #ccc'})\\\n    .hide(axis='index')\n\nwith open(html_colored_path, 'w', encoding='utf-8') as f:\n    f.write(f\"<h3>Sample Predictions (Color-Coded)</h3>\\n{styled_sample.to_html()}\")\n\ndisplay(HTML(\"<h3>Sample Predictions (Color-Coded)</h3>\"))\ndisplay(styled_sample)\n\n# ============================ #\n#   9. Log Artifacts to W&B   #\n# ============================ #\nartifact = wandb.Artifact('predictions_attention', type='predictions')\nartifact.add_file(csv_path)\nartifact.add_file(html_plain_path)\nartifact.add_file(html_colored_path)\nfor i in range(viz_count):\n    artifact.add_file(f'/kaggle/working/attent_@_sample_{i}.png')\nif viz_count > 0:\n    artifact.add_file(combined_path)\nwandb.log_artifact(artifact)\n\nwandb.log({\n    \"sample_predictions_table\": wandb.Html(html_colored_path),\n    \"attention_samples_combined\": wandb.Image(combined_path)\n})\n\nwandb.finish()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T11:34:38.291775Z","iopub.execute_input":"2025-05-20T11:34:38.292625Z","iopub.status.idle":"2025-05-20T11:38:10.929246Z","shell.execute_reply.started":"2025-05-20T11:34:38.292569Z","shell.execute_reply":"2025-05-20T11:38:10.928753Z"}},"outputs":[{"name":"stdout","text":"Using Devanagari font: Noto Sans Devanagari\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_113438-tgcf9h1w</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN/runs/tgcf9h1w' target=\"_blank\">test-inference-attention</a></strong> to <a href='https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN' target=\"_blank\">https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN/runs/tgcf9h1w' target=\"_blank\">https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN/runs/tgcf9h1w</a>"},"metadata":{}},{"name":"stdout","text":"Saved combined attention image: /kaggle/working/attent_@_samples_combined.png\n\nWord Accuracy: 42.71%\nChar Accuracy: 74.01%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<h3>Sample Predictions (Color-Coded)</h3>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x7cd1986b7690>","text/html":"<style type=\"text/css\">\n#T_6e058 th {\n  text-align: center;\n}\n#T_6e058_row0_col0, #T_6e058_row0_col1, #T_6e058_row0_col2, #T_6e058_row0_col3, #T_6e058_row0_col4, #T_6e058_row0_col5, #T_6e058_row2_col0, #T_6e058_row2_col1, #T_6e058_row2_col2, #T_6e058_row2_col3, #T_6e058_row2_col4, #T_6e058_row2_col5, #T_6e058_row5_col0, #T_6e058_row5_col1, #T_6e058_row5_col2, #T_6e058_row5_col3, #T_6e058_row5_col4, #T_6e058_row5_col5, #T_6e058_row7_col0, #T_6e058_row7_col1, #T_6e058_row7_col2, #T_6e058_row7_col3, #T_6e058_row7_col4, #T_6e058_row7_col5, #T_6e058_row8_col0, #T_6e058_row8_col1, #T_6e058_row8_col2, #T_6e058_row8_col3, #T_6e058_row8_col4, #T_6e058_row8_col5 {\n  background-color: #d4edda;\n  text-align: left;\n  padding: 8px;\n  font-size: 14px;\n  border: 1px solid #ccc;\n}\n#T_6e058_row1_col0, #T_6e058_row1_col1, #T_6e058_row1_col2, #T_6e058_row1_col3, #T_6e058_row1_col4, #T_6e058_row1_col5, #T_6e058_row3_col0, #T_6e058_row3_col1, #T_6e058_row3_col2, #T_6e058_row3_col3, #T_6e058_row3_col4, #T_6e058_row3_col5, #T_6e058_row4_col0, #T_6e058_row4_col1, #T_6e058_row4_col2, #T_6e058_row4_col3, #T_6e058_row4_col4, #T_6e058_row4_col5, #T_6e058_row6_col0, #T_6e058_row6_col1, #T_6e058_row6_col2, #T_6e058_row6_col3, #T_6e058_row6_col4, #T_6e058_row6_col5, #T_6e058_row9_col0, #T_6e058_row9_col1, #T_6e058_row9_col2, #T_6e058_row9_col3, #T_6e058_row9_col4, #T_6e058_row9_col5 {\n  background-color: #f8d7da;\n  text-align: left;\n  padding: 8px;\n  font-size: 14px;\n  border: 1px solid #ccc;\n}\n</style>\n<table id=\"T_6e058\">\n  <thead>\n    <tr>\n      <th id=\"T_6e058_level0_col0\" class=\"col_heading level0 col0\" >input</th>\n      <th id=\"T_6e058_level0_col1\" class=\"col_heading level0 col1\" >target</th>\n      <th id=\"T_6e058_level0_col2\" class=\"col_heading level0 col2\" >prediction</th>\n      <th id=\"T_6e058_level0_col3\" class=\"col_heading level0 col3\" >correct_word</th>\n      <th id=\"T_6e058_level0_col4\" class=\"col_heading level0 col4\" >correct_chars</th>\n      <th id=\"T_6e058_level0_col5\" class=\"col_heading level0 col5\" >total_chars</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_6e058_row0_col0\" class=\"data row0 col0\" >ank</td>\n      <td id=\"T_6e058_row0_col1\" class=\"data row0 col1\" >अंक</td>\n      <td id=\"T_6e058_row0_col2\" class=\"data row0 col2\" >अंक</td>\n      <td id=\"T_6e058_row0_col3\" class=\"data row0 col3\" >True</td>\n      <td id=\"T_6e058_row0_col4\" class=\"data row0 col4\" >3</td>\n      <td id=\"T_6e058_row0_col5\" class=\"data row0 col5\" >3</td>\n    </tr>\n    <tr>\n      <td id=\"T_6e058_row1_col0\" class=\"data row1 col0\" >anka</td>\n      <td id=\"T_6e058_row1_col1\" class=\"data row1 col1\" >अंक</td>\n      <td id=\"T_6e058_row1_col2\" class=\"data row1 col2\" >अंका</td>\n      <td id=\"T_6e058_row1_col3\" class=\"data row1 col3\" >False</td>\n      <td id=\"T_6e058_row1_col4\" class=\"data row1 col4\" >3</td>\n      <td id=\"T_6e058_row1_col5\" class=\"data row1 col5\" >3</td>\n    </tr>\n    <tr>\n      <td id=\"T_6e058_row2_col0\" class=\"data row2 col0\" >ankit</td>\n      <td id=\"T_6e058_row2_col1\" class=\"data row2 col1\" >अंकित</td>\n      <td id=\"T_6e058_row2_col2\" class=\"data row2 col2\" >अंकित</td>\n      <td id=\"T_6e058_row2_col3\" class=\"data row2 col3\" >True</td>\n      <td id=\"T_6e058_row2_col4\" class=\"data row2 col4\" >5</td>\n      <td id=\"T_6e058_row2_col5\" class=\"data row2 col5\" >5</td>\n    </tr>\n    <tr>\n      <td id=\"T_6e058_row3_col0\" class=\"data row3 col0\" >anakon</td>\n      <td id=\"T_6e058_row3_col1\" class=\"data row3 col1\" >अंकों</td>\n      <td id=\"T_6e058_row3_col2\" class=\"data row3 col2\" >अनकों</td>\n      <td id=\"T_6e058_row3_col3\" class=\"data row3 col3\" >False</td>\n      <td id=\"T_6e058_row3_col4\" class=\"data row3 col4\" >4</td>\n      <td id=\"T_6e058_row3_col5\" class=\"data row3 col5\" >5</td>\n    </tr>\n    <tr>\n      <td id=\"T_6e058_row4_col0\" class=\"data row4 col0\" >ankhon</td>\n      <td id=\"T_6e058_row4_col1\" class=\"data row4 col1\" >अंकों</td>\n      <td id=\"T_6e058_row4_col2\" class=\"data row4 col2\" >अंखों</td>\n      <td id=\"T_6e058_row4_col3\" class=\"data row4 col3\" >False</td>\n      <td id=\"T_6e058_row4_col4\" class=\"data row4 col4\" >4</td>\n      <td id=\"T_6e058_row4_col5\" class=\"data row4 col5\" >5</td>\n    </tr>\n    <tr>\n      <td id=\"T_6e058_row5_col0\" class=\"data row5 col0\" >ankon</td>\n      <td id=\"T_6e058_row5_col1\" class=\"data row5 col1\" >अंकों</td>\n      <td id=\"T_6e058_row5_col2\" class=\"data row5 col2\" >अंकों</td>\n      <td id=\"T_6e058_row5_col3\" class=\"data row5 col3\" >True</td>\n      <td id=\"T_6e058_row5_col4\" class=\"data row5 col4\" >5</td>\n      <td id=\"T_6e058_row5_col5\" class=\"data row5 col5\" >5</td>\n    </tr>\n    <tr>\n      <td id=\"T_6e058_row6_col0\" class=\"data row6 col0\" >angkor</td>\n      <td id=\"T_6e058_row6_col1\" class=\"data row6 col1\" >अंकोर</td>\n      <td id=\"T_6e058_row6_col2\" class=\"data row6 col2\" >अंगकॉर</td>\n      <td id=\"T_6e058_row6_col3\" class=\"data row6 col3\" >False</td>\n      <td id=\"T_6e058_row6_col4\" class=\"data row6 col4\" >2</td>\n      <td id=\"T_6e058_row6_col5\" class=\"data row6 col5\" >5</td>\n    </tr>\n    <tr>\n      <td id=\"T_6e058_row7_col0\" class=\"data row7 col0\" >ankor</td>\n      <td id=\"T_6e058_row7_col1\" class=\"data row7 col1\" >अंकोर</td>\n      <td id=\"T_6e058_row7_col2\" class=\"data row7 col2\" >अंकोर</td>\n      <td id=\"T_6e058_row7_col3\" class=\"data row7 col3\" >True</td>\n      <td id=\"T_6e058_row7_col4\" class=\"data row7 col4\" >5</td>\n      <td id=\"T_6e058_row7_col5\" class=\"data row7 col5\" >5</td>\n    </tr>\n    <tr>\n      <td id=\"T_6e058_row8_col0\" class=\"data row8 col0\" >angaarak</td>\n      <td id=\"T_6e058_row8_col1\" class=\"data row8 col1\" >अंगारक</td>\n      <td id=\"T_6e058_row8_col2\" class=\"data row8 col2\" >अंगारक</td>\n      <td id=\"T_6e058_row8_col3\" class=\"data row8 col3\" >True</td>\n      <td id=\"T_6e058_row8_col4\" class=\"data row8 col4\" >6</td>\n      <td id=\"T_6e058_row8_col5\" class=\"data row8 col5\" >6</td>\n    </tr>\n    <tr>\n      <td id=\"T_6e058_row9_col0\" class=\"data row9 col0\" >angarak</td>\n      <td id=\"T_6e058_row9_col1\" class=\"data row9 col1\" >अंगारक</td>\n      <td id=\"T_6e058_row9_col2\" class=\"data row9 col2\" >अंगरक</td>\n      <td id=\"T_6e058_row9_col3\" class=\"data row9 col3\" >False</td>\n      <td id=\"T_6e058_row9_col4\" class=\"data row9 col4\" >3</td>\n      <td id=\"T_6e058_row9_col5\" class=\"data row9 col5\" >6</td>\n    </tr>\n  </tbody>\n</table>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_char_accuracy</td><td>▁</td></tr><tr><td>test_word_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_char_accuracy</td><td>74.00953</td></tr><tr><td>test_word_accuracy</td><td>42.71435</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">test-inference-attention</strong> at: <a href='https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN/runs/tgcf9h1w' target=\"_blank\">https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN/runs/tgcf9h1w</a><br> View project at: <a href='https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN' target=\"_blank\">https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN</a><br>Synced 5 W&B file(s), 13 media file(s), 15 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_113438-tgcf9h1w/logs</code>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Heatmap Visulization","metadata":{}},{"cell_type":"code","source":"import wandb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\nwandb.init(project=\"DL_ASSIGNMENT_3_RNN\", name=\"attention_heatmaps\")\n\nnum_examples = 9\nfig, axes = plt.subplots(3, 3, figsize=(18, 18))\n\nfor i, ax in enumerate(axes.flat[:num_examples]):\n    src_tensor, tgt_tensor = test_dataset[i]\n    src_tensor = src_tensor.unsqueeze(0).to(device)\n\n    # Get prediction and attention weights\n    pred_seq, attn_weights = model.predict(src_tensor, max_len=30, beam_size=beam_size)\n\n    # Decode input tokens (exclude padding and EOS)\n    src_indices = src_tensor[0].cpu().tolist()\n    if input_vocab.get('<EOS>') in src_indices:\n        src_indices = src_indices[:src_indices.index(input_vocab['<EOS>'])]\n    input_tokens = [inv_input_vocab.get(idx, '?') for idx in src_indices if idx != 0]\n\n    # Decode output tokens\n    pred_indices = pred_seq.cpu().tolist()\n    if output_vocab.get('<EOS>') in pred_indices:\n        pred_indices = pred_indices[:pred_indices.index(output_vocab['<EOS>'])]\n    output_tokens = [inv_output_vocab.get(idx, '?') for idx in pred_indices]\n\n    # Stack attention weights list into 2D numpy array\n    attn_array = torch.stack(attn_weights).squeeze(1).cpu().detach().numpy()\n\n    # Slice to valid lengths (output tokens × input tokens)\n    attn = attn_array[:len(output_tokens), :len(input_tokens)]\n\n    # Plot attention heatmap on subplot\n    im = ax.imshow(attn, cmap='viridis')\n\n    ax.set_xticks(np.arange(len(input_tokens)))\n    ax.set_xticklabels(input_tokens, rotation=45, fontsize=12)\n    ax.set_yticks(np.arange(len(output_tokens)))\n    ax.set_yticklabels(output_tokens, fontsize=12)\n\n    ax.set_xlabel('Input (English)')\n    ax.set_ylabel('Output (Hindi)')\n\n    # Add input and output sequences as text above heatmap (wrapped for readability)\n    input_sentence = \" \".join(input_tokens)\n    output_sentence = \" \".join(output_tokens)\n    ax.set_title(f'Sample {i+1}\\nInput: {input_sentence}\\nOutput: {output_sentence}', fontsize=10)\n\n    # Log individual heatmap figure to wandb\n    fig_single, ax_single = plt.subplots(figsize=(6, 6))\n    im_single = ax_single.imshow(attn, cmap='viridis')\n    ax_single.set_xticks(np.arange(len(input_tokens)))\n    ax_single.set_xticklabels(input_tokens, rotation=45, fontsize=10)\n    ax_single.set_yticks(np.arange(len(output_tokens)))\n    ax_single.set_yticklabels(output_tokens, fontsize=10)\n    ax_single.set_xlabel('Input (English)')\n    ax_single.set_ylabel('Output (Hindi)')\n    ax_single.set_title(f'Sample {i+1}\\nInput: {input_sentence}\\nOutput: {output_sentence}', fontsize=10)\n    plt.tight_layout()\n\n    wandb.log({f\"attention_heatmap_sample_{i+1}\": wandb.Image(fig_single)})\n    plt.close(fig_single)\n\n# Finalize the grid figure\nplt.subplots_adjust(right=0.85, wspace=0.3, hspace=0.5)\nfig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.6, location='right')\n\n# Log entire grid as one image to wandb\nwandb.log({\"attention_heatmaps_grid\": wandb.Image(fig)})\nplt.close(fig)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T11:40:14.019220Z","iopub.execute_input":"2025-05-20T11:40:14.019653Z","iopub.status.idle":"2025-05-20T11:40:22.147693Z","shell.execute_reply.started":"2025-05-20T11:40:14.019629Z","shell.execute_reply":"2025-05-20T11:40:22.147114Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_114014-n3dnbk58</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN/runs/n3dnbk58' target=\"_blank\">attention_heatmaps</a></strong> to <a href='https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN' target=\"_blank\">https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN/runs/n3dnbk58' target=\"_blank\">https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN/runs/n3dnbk58</a>"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"# Connectivity Visualization Of attention","metadata":{}},{"cell_type":"code","source":"# Animation\nimport os\nimport torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom matplotlib import font_manager, rcParams\nfrom matplotlib import animation\nimport wandb\n\n# ============================ #\n#   1. Font Setup for Hindi   #\n# ============================ #\nfont_path = \"/kaggle/input/notosans-devanagiri/static/NotoSansDevanagari-Regular.ttf\"\nassert os.path.exists(font_path), \"Devanagari font not found.\"\ndevanagari_font = font_manager.FontProperties(fname=font_path)\nfont_manager.fontManager.addfont(font_path)\nrcParams['font.family'] = devanagari_font.get_name()\nprint(f\"Using Devanagari font: {devanagari_font.get_name()}\")\n\n# ============================ #\n#   2. Device & W&B Init      #\n# ============================ #\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nwandb.init(project=\"DL_ASSIGNMENT_3_RNN\", name=\"test-inference-attention-animated\")\n\n# ============================ #\n#   3. Load Data & Vocab      #\n# ============================ #\ntrain_path = '/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv'\ntest_path = '/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv'\ntrain_data = pd.read_csv(train_path, sep='\\t', header=None, dtype=str).dropna()\ntest_data = pd.read_csv(test_path, sep='\\t', header=None, dtype=str).dropna()\n\ninput_vocab = create_vocab(train_data, 1)\noutput_vocab = create_vocab(train_data, 0)\ninv_input_vocab = {v: k for k, v in input_vocab.items()}\ninv_output_vocab = {v: k for k, v in output_vocab.items()}\n\ntest_dataset = DakshinaDataset(test_data, input_vocab, output_vocab)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=pad_collate)\n\n# ============================ #\n#   4. Load Trained Model     #\n# ============================ #\nemb_dim = 32\nhidden_dim = 256\nnum_layers = 3\ndropout = 0.3\ncell_type = 'LSTM'\nbeam_size = 3\nsos_idx = output_vocab['<SOS>']\neos_idx = output_vocab['<EOS>']\n\nattention = Attention(hidden_dim)\nencoder = Attn_Encoder(len(input_vocab), emb_dim, hidden_dim, num_layers, cell_type, dropout)\ndecoder = Attn_Decoder(len(output_vocab), emb_dim, hidden_dim, num_layers, cell_type, dropout)\nmodel = Attn_Seq2Seq(encoder, decoder, device, sos_idx, eos_idx).to(device)\n\nmodel_path = '/kaggle/working/Attn_best_model.pt'\nmodel.load_state_dict(torch.load(model_path, map_location=device))\nmodel.eval()\n\n# ============================ #\n#  5. Attention Animation     #\n# ============================ #\nfrom matplotlib.patches import PathPatch\nfrom matplotlib.path import Path\nfrom matplotlib.patches import PathPatch\nfrom matplotlib.path import Path\n\ndef generate_attention_animation(input_chars, pred_chars, attn_weights, devanagari_font, save_path):\n    fig, ax = plt.subplots(figsize=(max(6, 0.7 * len(input_chars)), 4))\n    ax.set_xlim(-1, len(input_chars))\n    ax.set_ylim(-2, 2)\n    ax.axis('off')\n\n    # Plot input characters on top\n    input_positions = np.arange(len(input_chars))\n    for i, ch in enumerate(input_chars):\n        ax.text(i, 1.2, ch, fontsize=14, ha='center', fontproperties=devanagari_font)\n\n    # Predicted char (changing per frame)\n    pred_text = ax.text(len(input_chars)//2, -1.6, '', fontsize=16, ha='center', fontproperties=devanagari_font)\n\n    # Create curves from bottom (pred char) to each input char\n    curves = []\n    for i in range(len(input_chars)):\n        path_data = [\n            (Path.MOVETO, (len(input_chars)//2, -1.2)),  # start: prediction char\n            (Path.CURVE3, ((i + len(input_chars)//2) / 2, 0.0)),  # control point\n            (Path.CURVE3, (i, 0.8))  # end: input char\n        ]\n        path = Path([p for _, p in path_data], [cmd for cmd, _ in path_data])\n        patch = PathPatch(path, lw=0.5, alpha=0.0, color='green')\n        ax.add_patch(patch)\n        curves.append(patch)\n\n    def init():\n        for curve in curves:\n            curve.set_alpha(0.0)\n        pred_text.set_text('')\n        return curves + [pred_text]\n\n    def animate(t):\n        for i, curve in enumerate(curves):\n            alpha = attn_weights[t][i]\n            curve.set_alpha(alpha)\n            curve.set_linewidth(2.5 * alpha)\n        pred_text.set_text(f\"{pred_chars[t]}\")\n        return curves + [pred_text]\n\n    anim = animation.FuncAnimation(fig, animate, init_func=init,\n                                   frames=len(pred_chars), interval=800, blit=True)\n    anim.save(save_path, writer='pillow', dpi=100)\n    plt.close(fig)\n\n\n# ============================ #\n#     6. Inference & Log      #\n# ============================ #\npredictions = []\nviz_count, viz_samples = 0, 10\nanimation_info = []\n\nwith torch.no_grad():\n    for batch_idx, (src, tgt) in enumerate(test_loader):\n        src, tgt = src.to(device), tgt.to(device)\n        for i in range(src.size(0)):\n            src_single = src[i].unsqueeze(0)\n            tgt_single = tgt[i].unsqueeze(0)\n\n            pred_seq, best_attn_list = model.predict(src_single, max_len=30, beam_size=beam_size)\n\n            pred_indices = pred_seq.tolist()\n            if eos_idx in pred_indices:\n                pred_indices = pred_indices[:pred_indices.index(eos_idx)]\n            pred_str = ''.join([inv_output_vocab.get(idx, '?') for idx in pred_indices])\n\n            tgt_indices = tgt_single[0, 1:].tolist()\n            if eos_idx in tgt_indices:\n                tgt_indices = tgt_indices[:tgt_indices.index(eos_idx)]\n            tgt_str = ''.join([inv_output_vocab.get(idx, '?') for idx in tgt_indices])\n\n            input_indices = src_single[0].tolist()\n            input_str = ''.join([inv_input_vocab.get(idx, '?') for idx in input_indices if idx not in [0, input_vocab['<EOS>']]])\n\n            predictions.append({\n                'input': input_str,\n                'target': tgt_str,\n                'prediction': pred_str\n            })\n\n            if viz_count < viz_samples:\n                input_chars = [inv_input_vocab.get(idx, '?') for idx in input_indices if idx not in [0, input_vocab['<EOS>']]]\n                pred_chars = [inv_output_vocab.get(idx, '?') for idx in pred_indices]\n                attn_weights = [attn.cpu().numpy()[0] for attn in best_attn_list]\n\n                anim_path = f'/kaggle/working/attention_samples_{viz_count}.gif'\n                generate_attention_animation(input_chars, pred_chars, attn_weights, devanagari_font, anim_path)\n\n                wandb.save(anim_path)\n                wandb.log({f\"attention_samples_{viz_count}\": wandb.Video(anim_path)})\n\n                # Print or log words to stdout / console\n                print(f\"[{viz_count}] Input: {input_str} | Target: {tgt_str} | Prediction: {pred_str}\")\n\n                # Save animation info for HTML\n                animation_info.append({\n                    'gif_path': os.path.basename(anim_path),\n                    'input': input_str,\n                    'prediction': pred_str,\n                    'target': tgt_str\n                })\n\n                viz_count += 1\n        if viz_count >= viz_samples:\n            break\n\n\n# ============================ #\n#   7. Save CSV Predictions   #\n# ============================ #\npd.DataFrame(predictions).to_csv(\"/kaggle/working/predictions_with_attn.tsv\", index=False)\n\n# ============================ #\n#   8. Create HTML Grid       #\n# ============================ #\nimport base64\n\ndef embed_gif_base64(gif_path):\n    with open(gif_path, \"rb\") as f:\n        encoded = base64.b64encode(f.read()).decode('utf-8')\n    return f\"data:image/gif;base64,{encoded}\"\n\nhtml_str = '<h3>Attention Animations (Input → Prediction)</h3><table><tr>'\n\nfor i, anim in enumerate(animation_info):\n    if i % 5 == 0 and i > 0:\n        html_str += '</tr><tr>'  # new row after 5 columns\n\n    escaped_input = anim['input'].replace('<', '&lt;').replace('>', '&gt;')\n    escaped_pred = anim['prediction'].replace('<', '&lt;').replace('>', '&gt;')\n    gif_path = f\"/kaggle/working/{anim['gif_path']}\"\n\n    base64_img = embed_gif_base64(gif_path)\n\n    html_str += f'''\n    <td style=\"text-align:center;\">\n        <img src=\"{base64_img}\" width=\"200\"><br>\n        <span style=\"font-size:14px;\">{escaped_input} → {escaped_pred}</span>\n    </td>\n    '''\n\nhtml_str += '</tr></table>'\n\n# Save HTML to file\nhtml_file_path = \"/kaggle/working/attention_grids1.html\"\nwith open(html_file_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(html_str)\n\n# Log to W&B\nwandb.log({\"attention_grids1\": wandb.Html(html_str)})\n\n\nwandb.finish()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T11:43:26.354459Z","iopub.execute_input":"2025-05-20T11:43:26.355175Z","iopub.status.idle":"2025-05-20T11:43:36.245751Z","shell.execute_reply.started":"2025-05-20T11:43:26.355151Z","shell.execute_reply":"2025-05-20T11:43:36.245171Z"}},"outputs":[{"name":"stdout","text":"Using Devanagari font: Noto Sans Devanagari\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_114326-qgy0udki</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN/runs/qgy0udki' target=\"_blank\">test-inference-attention-animated</a></strong> to <a href='https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN' target=\"_blank\">https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN/runs/qgy0udki' target=\"_blank\">https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN/runs/qgy0udki</a>"},"metadata":{}},{"name":"stdout","text":"[0] Input: ank | Target: अंक | Prediction: अंक\n[1] Input: anka | Target: अंक | Prediction: अंका\n[2] Input: ankit | Target: अंकित | Prediction: अंकित\n[3] Input: anakon | Target: अंकों | Prediction: अनकों\n[4] Input: ankhon | Target: अंकों | Prediction: अंखों\n[5] Input: ankon | Target: अंकों | Prediction: अंकों\n[6] Input: angkor | Target: अंकोर | Prediction: अंगकॉर\n[7] Input: ankor | Target: अंकोर | Prediction: अंकोर\n[8] Input: angaarak | Target: अंगारक | Prediction: अंगारक\n[9] Input: angarak | Target: अंगारक | Prediction: अंगरक\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">test-inference-attention-animated</strong> at: <a href='https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN/runs/qgy0udki' target=\"_blank\">https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN/runs/qgy0udki</a><br> View project at: <a href='https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN' target=\"_blank\">https://wandb.ai/ma24m004-iit-madras/DL_ASSIGNMENT_3_RNN</a><br>Synced 5 W&B file(s), 11 media file(s), 0 artifact file(s) and 10 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_114326-qgy0udki/logs</code>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}