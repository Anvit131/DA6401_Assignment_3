# DA6401_Assignment_3
# Seq2Seq Transliteration Model with and without Attention (PyTorch)

This repository implements a Sequence-to-Sequence (Seq2Seq) model with and without an Attention both mechanisms for transliteration tasks using PyTorch. It supports Beam Search decoding and experiment tracking with Weights & Biases (wandb).

Dataset: Dakshina Indic Transliteration Dataset.

---

## ğŸ“ Project Structure

My model structure **without attention**

```text
â”œâ”€â”€ requirements.txt

â”œâ”€â”€ data/

   â””â”€â”€ DakshinaDataset

   â””â”€â”€ create_vocab

   â””â”€â”€   pad_collate

â”œâ”€â”€ Vanila_model

    â”œâ”€â”€ Encoder

    |   â””â”€â”€ forward

    â”œâ”€â”€ Decoder

        â””â”€â”€ forward

    â”œâ”€â”€ Seq2Seq

        â””â”€â”€ forward

        â””â”€â”€ predict

           â””â”€â”€beam_search

    â”œâ”€â”€train_model

    â”œâ”€â”€test model

```
My model structure **with attention**

```text

â”œâ”€â”€ model_with_attention
    â”œâ”€â”€ Attn_Encoder
    |   â””â”€â”€ forward
    â”œâ”€â”€ Attn_Decoder
        â””â”€â”€ forward
    â”œâ”€â”€ Attn_Seq2Seq
        â””â”€â”€ forward
        â””â”€â”€ predict
            â””â”€â”€beam_search
    â”œâ”€â”€Attn_train_model
    â”œâ”€â”€test model with attention
```

##  Features
- Encoder-Decoder Seq2Seq architecture
- Attention mechanism
- Beam Search decoding for better predictions
- Character-level and Word-level accuracy metrics
- Weights & Biases integration for logging
- Early stopping and model checkpointing

---

## Hyperparameter Sweep (wandb)

1. Initialize the sweep:
   wandb sweep

2. Run sweep agent:
   wandb agent 

---

##  Requirements

Contents of `requirements.txt`:

torch
torchvision
numpy
pandas 
scikit-learn 
wandb 
matplotlib
sentencepiece 

---

##  Credits

Developed by [Anvit Kumar]  
For [DA6410 / Deep Learning]  
